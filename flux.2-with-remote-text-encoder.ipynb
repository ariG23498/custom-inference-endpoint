{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariG23498/custom-inference-endpoint/blob/main/flux.2-with-remote-text-encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and Setup"
      ],
      "metadata": {
        "id": "EL8Wd56CxkRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -qq git+https://github.com/huggingface/diffusers\n",
        "!pip install --upgrade -qq bitsandbytes"
      ],
      "metadata": {
        "id": "BDbv7So9xZCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import Flux2Pipeline, Flux2Transformer2DModel\n",
        "from diffusers import BitsAndBytesConfig as DiffBitsAndBytesConfig\n",
        "from huggingface_hub import get_token\n",
        "import requests\n",
        "import torch\n",
        "import io"
      ],
      "metadata": {
        "id": "XYCKZKpgxmoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import diffusers\n",
        "import torch\n",
        "\n",
        "print(f\"{torch.__version__=}\")\n",
        "print(f\"{diffusers.__version__=}\")"
      ],
      "metadata": {
        "id": "UcNwRHgexpRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "print(f\"Total VRAM: {torch.cuda.get_device_properties().total_memory // 1024**3} GBs\")"
      ],
      "metadata": {
        "id": "3vzcMAnOx32r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference"
      ],
      "metadata": {
        "id": "HXOMXZ3XyaPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"black-forest-labs/FLUX.2-dev\"\n",
        "\n",
        "quantized_dit_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\n",
        "dit = Flux2Transformer2DModel.from_pretrained(\n",
        "  quantized_dit_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16, device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "pipe = Flux2Pipeline.from_pretrained(\n",
        "  repo_id,\n",
        "  text_encoder=None,\n",
        "  transformer=dit,\n",
        "  torch_dtype=torch.bfloat16,\n",
        ")\n",
        "pipe.enable_model_cpu_offload()"
      ],
      "metadata": {
        "id": "Wyz7Ghsl3d-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remote_text_encoder(prompts: str | list[str]):\n",
        "    response = requests.post(\n",
        "        \"https://rhknk53jznw37un7.us-east-1.aws.endpoints.huggingface.cloud/predict\",\n",
        "        json={\"prompt\": prompts},\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {get_token()}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "    )\n",
        "    assert response.status_code == 200, f\"{response.status_code=}\"\n",
        "    prompt_embeds = torch.load(io.BytesIO(response.content))\n",
        "    return prompt_embeds.to(\"cuda\")\n",
        "\n",
        "print(\"Running remote text encoder ☁️\")\n",
        "prompt = \"a photo of a forest with mist swirling around the tree trunks. The word 'FLUX.2 in diffusers' is painted over it in big, red brush strokes with visible texture\"\n",
        "prompt_embeds = remote_text_encoder([prompt])\n",
        "print(\"Done ✅\")"
      ],
      "metadata": {
        "id": "3xOXz5Bt1Vye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "out = pipe(\n",
        "  prompt_embeds=prompt_embeds,\n",
        "  generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
        "  num_inference_steps=50, # 28 is a good trade-off\n",
        "  guidance_scale=4,\n",
        "  height=512,\n",
        "  width=512,\n",
        ")\n",
        "\n",
        "out.images[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}